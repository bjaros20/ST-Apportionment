#Attempt to Get Revenue in R
install.packages("readxl")
install.packages("tidyr")
install.packages("dplyr")
install.packages("did")
install.packages("tidyverse")
install.packages("lmtest")
library(readxl)
library(tidyr)
library(dplyr)
library(did)
library(tidyverse)
library(lmtest)
Census State and Local Data- Tax Foundation
# Census State and Local Data- Tax Foundation
install.packages("readxl")
install.packages("tidyr")
install.packages("dplyr")
install.packages("tidyverse")
install.packages("xlsx")
library(readxl)
library(tidyr)
library(dplyr)
library(tidyverse)
library(xlsx)
#Nebraska or "Direct Switchers DiD Approach"
#Run DiD with all the Direct Switchers (NE, MO, MS, CO, RI, ND, DE)
#Control Group is all States that Did not Switch by 2007.
#this includes: AK, HI, KS, NM, OK.  AL and MT between 2008-now became "Step
#-wise" switchers.  FL and MA had DWS over the whole period 1976-2022.
library(tidyr)
library(dplyr)
library(tidyverse)
# for robust standard error estimation
library(lmtest)
# To calculate correct vcov matrix with 2WFE
library(multiwayvcov)
# For a package way to do FE
library(plm)
#Fixest
install.packages("fixest")
library(fixest)
#Load Data Tables in R
install.packages("data.table")
# Get rate of change for tax bases and find elasticity of tax base w.r.t apportionment and rate changes.
#Redo Callaway and Sant'Anna From Summer
library(readxl)
library(tidyr)
library(dplyr)
library(did)
library(tidyverse)
library(lmtest)
library(caret)
library(ggplot2)
#New Directory
setwd("~/Documents/GitHub/ST_Apportionment/Rates_Of_Change")
#'  data sets, the jackknife option is the fastest, with the caveat that it is not recommended
#'  for SC).
#' @param replications, the number of bootstrap replications
#' @param ... Additional arguments (currently ignored).
#'
#' @references Dmitry Arkhangelsky, Susan Athey, David A. Hirshberg, Guido W. Imbens, and Stefan Wager.
#'  "Synthetic Difference in Differences". arXiv preprint arXiv:1812.09970, 2019.
#'
#' @method vcov synthdid_estimate
#' @export
vcov.synthdid_estimate = function(object,
method = c("bootstrap", "jackknife", "placebo"),
replications = 200, ...) {
method = match.arg(method)
if(method == 'bootstrap') {
se = bootstrap_se(object, replications)
} else if(method == 'jackknife') {
se = jackknife_se(object)
} else if(method == 'placebo') {
se = placebo_se(object, replications)
}
matrix(se^2)
}
#' Calculate the standard error of a synthetic diff in diff estimate. Deprecated. Use vcov.synthdid_estimate.
#' @param ... Any valid arguments for vcov.synthdid_estimate
#' @export synthdid_se
synthdid_se = function(...) { sqrt(vcov(...)) }
# The bootstrap se: Algorithm 2 of Arkhangelsky et al.
bootstrap_se = function(estimate, replications) { sqrt((replications-1)/replications) * sd(bootstrap_sample(estimate, replications)) }
bootstrap_sample = function(estimate, replications) {
setup = attr(estimate, 'setup')
opts = attr(estimate, 'opts')
weights = attr(estimate, 'weights')
if (setup$N0 == nrow(setup$Y) - 1) { return(NA) }
theta = function(ind) {
if(all(ind <= setup$N0) || all(ind > setup$N0)) { NA }
else {
weights.boot = weights
weights.boot$omega = sum_normalize(weights$omega[sort(ind[ind <= setup$N0])])
do.call(synthdid_estimate, c(list(Y=setup$Y[sort(ind),], N0=sum(ind <= setup$N0), T0=setup$T0, X=setup$X[sort(ind), ,], weights=weights.boot), opts))
}
}
bootstrap.estimates = rep(NA, replications)
count = 0
while(count < replications) {
bootstrap.estimates[count+1] = theta(sample(1:nrow(setup$Y), replace=TRUE))
if(!is.na(bootstrap.estimates[count+1])) { count = count+1 }
}
bootstrap.estimates
}
# The fixed-weights jackknife estimate of variance: Algorithm 3 of Arkhangelsky et al.
# if weights = NULL is passed explicitly, calculates the usual jackknife estimate of variance.
# returns NA if there is one treated unit or, for the fixed-weights jackknife, one control with nonzero weight
jackknife_se = function(estimate, weights = attr(estimate, 'weights')) {
setup = attr(estimate, 'setup')
opts = attr(estimate, 'opts')
if (!is.null(weights)) {
opts$update.omega = opts$update.lambda = FALSE
}
if (setup$N0 == nrow(setup$Y) - 1 || (!is.null(weights) && sum(weights$omega != 0) == 1)) { return(NA) }
theta = function(ind) {
weights.jk = weights
if (!is.null(weights)) { weights.jk$omega = sum_normalize(weights$omega[ind[ind <= setup$N0]]) }
estimate.jk = do.call(synthdid_estimate,
c(list(Y=setup$Y[ind, ], N0=sum(ind <= setup$N0), T0=setup$T0, X = setup$X[ind, , ], weights = weights.jk), opts))
}
jackknife(1:nrow(setup$Y), theta)
}
#' Jackknife standard error of function `theta` at samples `x`.
#' @param x vector of samples
#' @param theta a function which returns a scalar estimate
#' @importFrom stats var
#' @keywords internal
jackknife = function(x, theta) {
n = length(x)
u = rep(0, n)
for (i in 1:n) {
u[i] = theta(x[-i])
}
jack.se = sqrt(((n - 1) / n) * (n - 1) * var(u))
jack.se
}
# The placebo se: Algorithm 4 of Arkhangelsky et al.
placebo_se = function(estimate, replications) {
setup = attr(estimate, 'setup')
opts = attr(estimate, 'opts')
weights = attr(estimate, 'weights')
N1 = nrow(setup$Y) - setup$N0
if (setup$N0 <= N1) { stop('must have more controls than treated units to use the placebo se') }
theta = function(ind) {
N0 = length(ind)-N1
weights.boot = weights
weights.boot$omega = sum_normalize(weights$omega[ind[1:N0]])
do.call(synthdid_estimate, c(list(Y=setup$Y[ind,], N0=N0,  T0=setup$T0,  X=setup$X[ind, ,], weights=weights.boot), opts))
}
sqrt((replications-1)/replications) * sd(replicate(replications, theta(sample(1:setup$N0))))
}
sum_normalize = function(x) {
if(sum(x) != 0) { x / sum(x) }
else { rep(1/length(x), length(x)) }
# if given a vector of zeros, return uniform weights
# this fine when used in bootstrap and placebo standard errors, where it is used only for initialization
# for jackknife standard errors, where it isn't, we handle the case of a vector of zeros without calling this function.
}
#Run a one-sided Significance test
#Load Packages
library(tidyr)
library(dplyr)
library(tidyverse)
library(lmtest)
library(ggplot2)
library(did) # for running DiD
library(plm)
library(lmtest)
library(synthdid)
library(fixest)
library(boot)
library(ggthemes)
# set seed
set.seed(26)
#JMP Directory
setwd("~/Documents/GitHub/ST-Apportionment/job_market_data")
naive_ci<-read.csv("naive_ci.csv")
# Next thing to estimate, Real Corporate Income, base year 1983-1984
real_CI <- naive_ci%>%
mutate(real_ci = (naive_ci/CPI_def)*100)
real_CI_cap <- real_CI %>%
mutate(real_ci_cap = real_ci/population)
#Create base dataframe that has nat_share as dependent variable.
Filter_frac <-real_CI_cap %>%
select(State_Acronym,year,year_effective,State_Name,real_ci_cap,Post)
filt_Corp <-Filter_frac
#Create State Dataframes
#create result list to store dataframe
result_list <- list()
# Make a copy of the original dataframe to work with
original_df <- filt_Corp
#counter variable, so as loop progresses, drops first state
counter <- 1
while (TRUE) {
#Reset to original each start
df <-filt_Corp
# Arrange by year_effective and select the first state for treatment
df <- df %>% arrange(year_effective)
#counter variable for running the loop
if(df$year_effective[counter] >= 2022) {break}
treatment_state <- df %>% slice(counter)
treatment_year <- treatment_state$year_effective
treatment_state_name <- treatment_state$State_Name
#filter out prior treated states, but keep no treatment states
#first half of filter keeps no treatment states in,
df <- df %>%
filter(is.na(year_effective) | is.na(State_Acronym) | year_effective >= treatment_year)
# removes states treated in same year
#the year_effective=treatment_year are filtered out if they are not 'treatment_state'
df <- df %>%
filter(is.na(year_effective) | (!(State_Name != treatment_state_name & year_effective == treatment_year)))
# Filter out rows with year <= 2 years after treatment_year
df <- df %>%
filter(year <= treatment_year + 2)
#filter out any states that get treated within 2 years of treatment
df <- df %>%
filter(is.na(year_effective) | (!(year_effective > treatment_year & year_effective<= treatment_year + 2)))
#filter out Ohio after treatment_year >=2012, because OH eliminates CI in 2014
df <- df %>%
filter(!(treatment_year >= 2012 & State_Name == "Ohio"))
# Store the dataframe for this treatment state
assign(treatment_state_name, df)
result_list[[treatment_state_name]] <- df
# Check if the treatment year is 2022 or greater, break
if (treatment_year >= 2022) {break}
#increment counter
counter <-counter + 1
#empty dataframe break
if (nrow(df) == 0) {break}
}
# Initialize an empty list to store point estimates and statistics
point_estimate_list <- list()
# Loop over each dataframe in result_list
for (state_name in names(result_list)) {
# Access the dataframe
current_df <- result_list[[state_name]]
#Make the tibble from the result list a dataframe, easier for panel.matrics function
current_df <- as.data.frame(current_df)
#Drop states that have NA for ratio (like Alaska because no Income Tax)
current_df <- na.omit(current_df[, c("State_Acronym", "year", "real_ci_cap", "Post")])
#eliminate Inf lines and the state that has them for estimation, like Ohio 2009-2013
states_with_inf <- current_df %>%
group_by(State_Acronym) %>%
filter(any(is.infinite(real_ci_cap))) %>%
pull(State_Acronym) %>%
unique()
# Filter out those states from the dataframe
current_df <- current_df %>%
filter(!State_Acronym %in% states_with_inf)
# Create the panel matrices for sDiD using synthdid
current_sDiD <- panel.matrices(current_df, unit = "State_Acronym", time = "year", outcome = "real_ci_cap", treatment = "Post")
# Calculate the synthetic difference-in-differences estimate
current_tau_hat <- synthdid_estimate(current_sDiD$Y, current_sDiD$N0, current_sDiD$T0)
se <- sqrt(vcov(current_tau_hat, method = 'placebo'))
#  vcov.synthdid_estimate(current_tau_hat,method = 'bootstrap',replications = 500)
# Calculate the t-statistic
t_statistic <- as.numeric(current_tau_hat) / se
# Calculate the p-value
p_value <- 2 * pt(abs(t_statistic), df = nrow(current_df) - 1, lower.tail = FALSE)
# Print the point estimate, confidence interval, t-statistic, and p-value
cat(sprintf('State: %s\n', state_name))
cat(sprintf('Point estimate: %1.2f\n', current_tau_hat))
cat(sprintf('95%% CI (%1.2f, %1.2f)\n', current_tau_hat - 1.96 * se, current_tau_hat + 1.96 * se))
cat(sprintf('t-statistic: %1.3f\n', t_statistic))
cat(sprintf('p-value: %1.4f\n', p_value))
# Summary statistics
#  print(summary(current_tau_hat))
}
current_sDiD
current_tau_hat
bootstrap_se(current_tau_hat, replications = 500)
se <- sqrt(vcov(current_tau_hat, method = 'placebo'))
se
View(bootstrap_se)
View(bootstrap_sample)
setup
#'  data sets, the jackknife option is the fastest, with the caveat that it is not recommended
#'  for SC).
#' @param replications, the number of bootstrap replications
#' @param ... Additional arguments (currently ignored).
#'
#' @references Dmitry Arkhangelsky, Susan Athey, David A. Hirshberg, Guido W. Imbens, and Stefan Wager.
#'  "Synthetic Difference in Differences". arXiv preprint arXiv:1812.09970, 2019.
#'
#' @method vcov synthdid_estimate
#' @export
vcov.synthdid_estimate = function(object,
method = c("bootstrap", "jackknife", "placebo"),
replications = 200, ...) {
method = match.arg(method)
if(method == 'bootstrap') {
se = bootstrap_se(object, replications)
} else if(method == 'jackknife') {
se = jackknife_se(object)
} else if(method == 'placebo') {
se = placebo_se(object, replications)
}
matrix(se^2)
}
View(bootstrap_se)
View(bootstrap_sample)
#' Calculate the standard error of a synthetic diff in diff estimate. Deprecated. Use vcov.synthdid_estimate.
#' @param ... Any valid arguments for vcov.synthdid_estimate
#' @export synthdid_se
synthdid_se = function(...) { sqrt(vcov(...)) }
# The bootstrap se: Algorithm 2 of Arkhangelsky et al.
bootstrap_se = function(estimate, replications) { sqrt((replications-1)/replications) * sd(bootstrap_sample(estimate, replications)) }
bootstrap_sample = function(estimate, replications) {
current_sDiD = attr(estimate, 'current_sDiD')
opts = attr(estimate, 'opts')
weights = attr(estimate, 'weights')
if (current_sDiD$N0 == nrow(current_sDiD$Y) - 1) { return(NA) }
theta = function(ind) {
if(all(ind <= current_sDiD$N0) || all(ind > current_sDiD$N0)) { NA }
else {
weights.boot = weights
weights.boot$omega = sum_normalize(weights$omega[sort(ind[ind <= current_sDiD$N0])])
do.call(synthdid_estimate, c(list(Y=current_sDiD$Y[sort(ind),], N0=sum(ind <= current_sDiD$N0), T0=current_sDiD$T0, X=current_sDiD$X[sort(ind), ,], weights=weights.boot), opts))
}
}
bootstrap.estimates = rep(NA, replications)
count = 0
while(count < replications) {
bootstrap.estimates[count+1] = theta(sample(1:nrow(current_sDiD$Y), replace=TRUE))
if(!is.na(bootstrap.estimates[count+1])) { count = count+1 }
}
bootstrap.estimates
}
# The fixed-weights jackknife estimate of variance: Algorithm 3 of Arkhangelsky et al.
# if weights = NULL is passed explicitly, calculates the usual jackknife estimate of variance.
# returns NA if there is one treated unit or, for the fixed-weights jackknife, one control with nonzero weight
jackknife_se = function(estimate, weights = attr(estimate, 'weights')) {
current_sDiD = attr(estimate, 'current_sDiD')
opts = attr(estimate, 'opts')
if (!is.null(weights)) {
opts$update.omega = opts$update.lambda = FALSE
}
if (current_sDiD$N0 == nrow(current_sDiD$Y) - 1 || (!is.null(weights) && sum(weights$omega != 0) == 1)) { return(NA) }
theta = function(ind) {
weights.jk = weights
if (!is.null(weights)) { weights.jk$omega = sum_normalize(weights$omega[ind[ind <= current_sDiD$N0]]) }
estimate.jk = do.call(synthdid_estimate,
c(list(Y=current_sDiD$Y[ind, ], N0=sum(ind <= current_sDiD$N0), T0=current_sDiD$T0, X = current_sDiD$X[ind, , ], weights = weights.jk), opts))
}
jackknife(1:nrow(current_sDiD$Y), theta)
}
#' Jackknife standard error of function `theta` at samples `x`.
#' @param x vector of samples
#' @param theta a function which returns a scalar estimate
#' @importFrom stats var
#' @keywords internal
jackknife = function(x, theta) {
n = length(x)
u = rep(0, n)
for (i in 1:n) {
u[i] = theta(x[-i])
}
jack.se = sqrt(((n - 1) / n) * (n - 1) * var(u))
jack.se
}
# The placebo se: Algorithm 4 of Arkhangelsky et al.
placebo_se = function(estimate, replications) {
current_sDiD = attr(estimate, 'current_sDiD')
opts = attr(estimate, 'opts')
weights = attr(estimate, 'weights')
N1 = nrow(current_sDiD$Y) - current_sDiD$N0
if (current_sDiD$N0 <= N1) { stop('must have more controls than treated units to use the placebo se') }
theta = function(ind) {
N0 = length(ind)-N1
weights.boot = weights
weights.boot$omega = sum_normalize(weights$omega[ind[1:N0]])
do.call(synthdid_estimate, c(list(Y=current_sDiD$Y[ind,], N0=N0,  T0=current_sDiD$T0,  X=current_sDiD$X[ind, ,], weights=weights.boot), opts))
}
sqrt((replications-1)/replications) * sd(replicate(replications, theta(sample(1:current_sDiD$N0))))
}
sum_normalize = function(x) {
if(sum(x) != 0) { x / sum(x) }
else { rep(1/length(x), length(x)) }
# if given a vector of zeros, return uniform weights
# this fine when used in bootstrap and placebo standard errors, where it is used only for initialization
# for jackknife standard errors, where it isn't, we handle the case of a vector of zeros without calling this function.
}
View(bootstrap_sample)
bootstrap_se(current_tau_hat, replications = 500)
View(current_sDiD)
print(attr(estimate, 'current_sDiD'))
print(attr(current_tau_hat, 'current_sDiD'))
print(attributes(current_tau_hat))
current_tau_hat <- synthdid_estimate(current_sDiD$Y, current_sDiD$N0, current_sDiD$T0)
se <- sqrt(vcov(current_tau_hat, method = 'placebo', replications = 500))
current_tau_hat <- synthdid_estimate(current_sDiD$Y, current_sDiD$N0, current_sDiD$T0)
se <- sqrt(vcov(current_tau_hat, method = 'placebo', replications = 500))
se <- sqrt(vcov(current_tau_hat, method = 'placebo'))
# Main result: Naive, Real, Corporate Income per capita
#Empirical Approach, sDiD
# Care about the t-statistic, need to BOOTSTRAP with replace for normal distribution
#Run a one-sided Significance test
#Load Packages
library(tidyr)
library(dplyr)
library(tidyverse)
library(lmtest)
library(ggplot2)
library(did) # for running DiD
library(plm)
library(lmtest)
library(synthdid)
library(fixest)
library(boot)
library(ggthemes)
# set seed
set.seed(26)
#JMP Directory
setwd("~/Documents/GitHub/ST-Apportionment/job_market_data")
naive_ci<-read.csv("naive_ci.csv")
