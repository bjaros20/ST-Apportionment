#Attempt to Get Revenue in R
install.packages("readxl")
install.packages("tidyr")
install.packages("dplyr")
install.packages("did")
install.packages("tidyverse")
install.packages("lmtest")
library(readxl)
library(tidyr)
library(dplyr)
library(did)
library(tidyverse)
library(lmtest)
Census State and Local Data- Tax Foundation
# Census State and Local Data- Tax Foundation
install.packages("readxl")
install.packages("tidyr")
install.packages("dplyr")
install.packages("tidyverse")
install.packages("xlsx")
library(readxl)
library(tidyr)
library(dplyr)
library(tidyverse)
library(xlsx)
#Nebraska or "Direct Switchers DiD Approach"
#Run DiD with all the Direct Switchers (NE, MO, MS, CO, RI, ND, DE)
#Control Group is all States that Did not Switch by 2007.
#this includes: AK, HI, KS, NM, OK.  AL and MT between 2008-now became "Step
#-wise" switchers.  FL and MA had DWS over the whole period 1976-2022.
library(tidyr)
library(dplyr)
library(tidyverse)
# for robust standard error estimation
library(lmtest)
# To calculate correct vcov matrix with 2WFE
library(multiwayvcov)
# For a package way to do FE
library(plm)
#Fixest
install.packages("fixest")
library(fixest)
#Load Data Tables in R
install.packages("data.table")
# Get rate of change for tax bases and find elasticity of tax base w.r.t apportionment and rate changes.
#Redo Callaway and Sant'Anna From Summer
library(readxl)
library(tidyr)
library(dplyr)
library(did)
library(tidyverse)
library(lmtest)
library(caret)
library(ggplot2)
#New Directory
setwd("~/Documents/GitHub/ST_Apportionment/Rates_Of_Change")
#load packages
library(tidyr)
library(dplyr)
library(tidyverse)
#set working directory
setwd("~/Documents/GitHub/ST-Apportionment/CAEMP25N")
#Load data
bls <- read.csv("CAEMP25N__ALL_AREAS_2001_2022.csv")
View(bls)
#create vector of states and USA
state_names <- c("United States",
"Alabama", "Alaska", "Arizona", "Arkansas", "California",
"Colorado", "Connecticut", "Delaware", "Florida", "Georgia",
"Hawaii", "Idaho", "Illinois", "Indiana", "Iowa", "Kansas",
"Kentucky", "Louisiana", "Maine", "Maryland", "Massachusetts",
"Michigan", "Minnesota", "Mississippi", "Missouri", "Montana",
"Nebraska", "Nevada", "New Hampshire", "New Jersey", "New Mexico",
"New York", "North Carolina", "North Dakota", "Ohio", "Oklahoma",
"Oregon", "Pennsylvania", "Rhode Island", "South Carolina",
"South Dakota", "Tennessee", "Texas", "Utah", "Vermont",
"Virginia", "Washington", "West Virginia", "Wisconsin", "Wyoming")
#filter to only have bls values that are the US or state
filt_bls <- bls%>%
filter(GeoName %in% state_names)
View(filt_bls)
View(filt_bls)
str(filt_bls)
filt2_bls <- filt_bls %>%
select(-GeoFIPS,-TableName,-Unit)
#Remove the X from in front of year
colnames(filt2_bls) <- gsub("^X", "", colnames(filt2_bls))
View(filt2_bls)
str(filt2_bls)
#convert years to year column
filt2_bls_long <- filt2_bls %>%
pivot_longer(cols = starts_with("2"),  # This targets all columns from 2001 to 2022
names_to = "year",
values_to = "value")
View(filt2_bls_long)
#spread the values for sector out via wide split:
filt2_bls_wide <- filt2_bls_long %>%
pivot_wider(names_from = Description,
values_from = value)
View(filt2_bls_wide)
#collapse the NAs
filt2_bls_transposed <- filt2_bls_wide %>%
arrange(GeoName, year)
View(filt2_bls_transposed)
str
str()
str(filt2_bls_transposed)
#One more step, and it is going to be there.
write.csv(filt2_bls_transposed,"bls_transpose.csv")
View(filt2_bls_long)
View(filt2_bls_wide)
View(filt2_bls_long)
str(filt2_bls_wide)
write.csv(filt2_bls_wide,"bls_wide.csv")
clean <-filt2_bls_wide %>%
select(-LineCode,-IndustryClassification)
#collapse the NAs
clean_transpose <- clean %>%
arrange(GeoName, year)
View(clean_transpose)
clean_transpose <- clean %>%
arrange(GeoName)
View(clean_transpose)
str(clean_transpose)
#collapse the NAs
clean_transpose <- clean %>%
arrange(GeoName,Region,year)
clean_transpose_filled <- clean_transpose %>%
group_by(GeoName, year) %>%
fill(everything(), .direction = "downup") %>%
ungroup()
View(clean_transpose_filled)
#This gave 33 duplicate rows, now will use distinct to eliminate duplicate rows
clean_transpose_unique <- clean_transpose_filled %>%
distinct()
View(clean_transpose_unique)
write.csv(clean_transpose_unique,"bls_NoID_complete.csv")
